---
title: 2020-2-18/19 ctc+ace总结
tags: cnn,rnn,ctc,atttention,ace
grammar_cjkRuby: true
---


CNN+RNN+CTC文字识别方法：
框架：CRNN+CTC
1、CRNN：通过CNN网络提取文本图像的特征，然后通过RNN（LSTM）转换特征来识别具体的文本信息
CNN——卷积网络将输入图像的转换为特征图矩阵；
RNN——深层双向LSTM网络，在以上特征举证的基础上提取出文字的序列特征；
转录层——把LSTM的每帧预测输出转换成标签序列，得到具体的字符；
通常：用softmax 函数来处理LSTM的输出，然后通过选择最大值对应的字符
问题：
       softmax输出类别数目是固定的，而文本是不定长的，无法处理不定长的标签；
       softmax交叉熵损失，需要每列输出对应一个字符元素。即每个训练样本需要标记出各字符的位置，而现实情况是文本样本的输出位置和类别难以一一对应；
解决：CTC-loss，不需要逐个位置标注字符又可以输出任意长度label的方法来训练网络
2、CTC
原理：ctc引入的占位符来对齐输出，对于给定的输入，通过训练模型来最大化后验概率，利用梯度下降来训练模型；

ACE聚合交叉熵损失函数：
问题：
1、CTC的前向后向算法导致大量的计算消耗 。
2、attention通过注意模块对齐标签，产生额外的计算损耗
提出ACE损失函数：
1、通过对所有时间维数的第k类的概率求和，计算每一个类的字符数量；
2、第k个字符的累计概率标准化，并将标签标注为所有类的概率分布；
3、通过交叉熵比较2中的两个概率分布。
优点：
 1、计算更快，更少的内存消耗
 2、论文实现显示效果优于ctc
 3、 不需要一一对应的实例顺序序列标签信息


 
---
title: 2020-2-20 ace代码学习注释
tags: ace
grammar_cjkRuby: true
---

代码注释，环境配置（环境搭建后，内存不够，无法执行）
